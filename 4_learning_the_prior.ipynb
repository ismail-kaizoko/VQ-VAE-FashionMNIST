{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "# from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#vqvae libs\n",
    "from vqvae import VQVAE\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Visuals utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build first the dataset :\n",
    "\n",
    "The dataset to be train on will be the sequences of indices outputed by the quantization layer of the VQ-VAE of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    ",k,\n",
    "# 1. Load and Preprocess the Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),               # Convert images to PyTorch tensors\n",
    "    # transforms.Normalize((0.5,), (0.5,)) # Normalize the images to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the Fashion-MNIST training and test dataset\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "TrainLoader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "TestLoader  = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Class labels for reference\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1952009/1369189890.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_vq.load_state_dict(torch.load(model_path)['model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "K = 64\n",
    "D =  32 # dimension of each embedding vector\n",
    "in_channels = 1 # gray scale image  = 1 color channel\n",
    "downsampling_factor = 4 # two stages of dwonsampling the image (28x28) --> (7x7)\n",
    "\n",
    "model_path = 'saved_models/model_Refit.pth'\n",
    "\n",
    "model_vq = VQVAE(in_channels, D, K)\n",
    "model_vq.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "model_vq = model_vq.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to pass through the whole dataset, which results on \n",
    "\n",
    "dataset = []\n",
    "\n",
    "# Process the dataset\n",
    "with torch.no_grad():  # No need to track gradients\n",
    "    for (batch,_) in TrainLoader:\n",
    "        # Pass the batch through the encoder\n",
    "        indices = model_vq.get_indices(batch.to(device))  # Output shape: [B, HW = 7x7 = 49]\n",
    "        dataset.append(indices.cpu().numpy())\n",
    "\n",
    "# Concatenate all the latent vectors into a single NumPy array\n",
    "dataset = np.concatenate(dataset, axis=0)  # Shape: (size_of_dataset, 32*32)\n",
    "\n",
    "# # Optionally, save the latent vectors to disk\n",
    "# np.save('dataset.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44,  0, 37, 54, 49, 60, 57, 21, 17, 32, 45, 53, 17, 44, 12, 53, 14,\n",
       "        7,  3, 38, 21, 47,  3,  7,  7, 53, 14, 57, 12, 27,  7, 41, 14, 37,\n",
       "       63, 57,  7,  7, 14, 53, 53, 33, 44, 43, 37,  3, 24,  0, 57])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the latent vectors to disk\n",
    "np.save('sequences_dataset.npy', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VQVAECodebookDataset(Dataset):\n",
    "    def __init__(self, codebook_sequences, start_token_idx=K+1, end_token_idx=K+2):\n",
    "        \"\"\"\n",
    "        :param codebook_sequences: A list of sequences where each sequence is a list of integers \n",
    "                                   (indices from the VQ-VAE codebook, range 0 - K-1).\n",
    "        :param start_token_idx: Integer representing the [START] token (default: K).\n",
    "        :param end_token_idx: Integer representing the [END] token (default: K+1).\n",
    "        \"\"\"\n",
    "        self.codebook_sequences = codebook_sequences\n",
    "        self.start_token_idx = start_token_idx\n",
    "        self.end_token_idx = end_token_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.codebook_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.codebook_sequences[idx]\n",
    "        \n",
    "        # Add [START] and [END] tokens to the sequence\n",
    "        input_sequence = [self.start_token_idx] + sequence.tolist()  # [START] token at the beginning\n",
    "        target_sequence = sequence.tolist() + [self.end_token_idx]   # [END] token at the end\n",
    "        \n",
    "        # Return as PyTorch tensors (1D, dtype long)\n",
    "        return torch.tensor(input_sequence, dtype=torch.long), torch.tensor(target_sequence, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = VQVAECodebookDataset(dataset)\n",
    "TrainLoader = DataLoader(Dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Defining the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_heads, n_layers, sequence_length):\n",
    "        super(SmallTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # self.pos_embedding = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim))  # Positional Encoding\n",
    "        self.register_buffer(\"pos_embedding\", self.create_linear_positional_encoding(sequence_length, embedding_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def create_linear_positional_encoding(self, sequence_length, embedding_dim):\n",
    "        \"\"\"\n",
    "        Create a simple linear positional encoding where each position is scaled linearly.\n",
    "        \"\"\"\n",
    "        # Generate positions [0, 1, ..., sequence_length - 1]\n",
    "        positions = torch.arange(0, sequence_length).unsqueeze(1).float()  # Shape: [sequence_length, 1]\n",
    "        # Scale positions linearly to the embedding dimension\n",
    "        # Normalize by dividing by sequence_length to keep values small\n",
    "        encoding = positions / sequence_length  # Shape: [sequence_length, 1]\n",
    "        # Expand encoding to match the embedding dimension\n",
    "        encoding = encoding * torch.linspace(0, 1, embedding_dim).unsqueeze(0)  # Shape: [sequence_length, embedding_dim]\n",
    "        return encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed tokens + positions\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :x.size(1), :]\n",
    "\n",
    "        # Next time decoment this, to leverage the autoregressiveness\n",
    "        # # Create a causal mask to prevent attention to future tokens\n",
    "        # causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        # # Transformer forward pass with the mask\n",
    "        # x = self.transformer(x, src_mask=causal_mask)\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Output logits for each token position\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ids/ihamdaoui-21/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vocab_size = K+3 # num_embeddings of codebook plus the two special_tokens (either : [START] or [END] )\n",
    "embedding_dim = 32\n",
    "hidden_dim = embedding_dim*2\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "sequence_length = 50 # since my image are downsampled 2 times, 28x28 --> 7x7 = 49, plus the [END] or [START]\n",
    "\n",
    "lr = 1e-4\n",
    "num_epochs = 10\n",
    "# Model, loss, optimizer\n",
    "model = SmallTransformerModel(vocab_size, embedding_dim, hidden_dim, n_heads, n_layers, sequence_length)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 163.64batch/s]\n",
      "Epochs:  10%|█         | 1/10 [00:11<01:43, 11.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 164.08batch/s]\n",
      "Epochs:  20%|██        | 2/10 [00:22<01:31, 11.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 163.13batch/s]\n",
      "Epochs:  30%|███       | 3/10 [00:34<01:20, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.6671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 162.85batch/s]\n",
      "Epochs:  40%|████      | 4/10 [00:45<01:08, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 168.79batch/s]\n",
      "Epochs:  50%|█████     | 5/10 [00:57<00:56, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:10<00:00, 175.13batch/s]\n",
      "Epochs:  60%|██████    | 6/10 [01:07<00:44, 11.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:10<00:00, 176.30batch/s]\n",
      "Epochs:  70%|███████   | 7/10 [01:18<00:32, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 168.08batch/s]\n",
      "Epochs:  80%|████████  | 8/10 [01:29<00:22, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.6631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 165.07batch/s]\n",
      "Epochs:  90%|█████████ | 9/10 [01:40<00:11, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 154.90batch/s]\n",
      "Epochs: 100%|██████████| 10/10 [01:53<00:00, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.6623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 3e-5\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    with tqdm(enumerate(TrainLoader), unit=\"batch\", total=len(TrainLoader)) as tepoch:\n",
    "        for bacth in tepoch:\n",
    "    # Wrap the TrainLoader with tqdm for progress tracking within each epoch\n",
    "    # for batch in tqdm(TrainLoader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Reshape logits and targets for loss calculation\n",
    "            logits = logits.view(-1, vocab_size)  # Flatten for all time steps\n",
    "            y = y.view(-1)  # Flatten targets to match logits\n",
    "            \n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(TrainLoader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #from : https://medium.com/@ikim1994914/understanding-the-modern-llm-part-3-using-pytorch-built-in-function-to-build-an-autoregressive-3feeb14496e9\n",
    "# ########################## define transformer function ##########################\n",
    "# #################################################################################\n",
    "# class fullTransformer(nn.Module):\n",
    "#     def __init__(self, device, input_size, max_length_src, max_length_tgt, d_model, nhead = 8,\n",
    "#                  num_encoder_layers = 4, num_decoder_layers = 4,\n",
    "#                  dim_feedforward = 1024, dropout = 0.1, pad_idx = 50257):\n",
    "#         super(fullTransformer, self).__init__()\n",
    "#         self.device = device\n",
    "#         self.input_size = input_size # this is the # of the vocabularies in the source (how mnay tokens the tokenizer has)\n",
    "#         self.output_size = input_size # input tokenizer and output tokenizer are the same, so input_size = output_size\n",
    "#         self.d_model = d_model # this is the hidden, or the embedding dimension\n",
    "#         self.nhead = nhead # number of multihead-attention \n",
    "#         self.enc_layer = num_encoder_layers\n",
    "#         self.dec_layer = num_decoder_layers\n",
    "#         self.dim_forward = dim_feedforward\n",
    "#         self.dropout = dropout\n",
    "#         self.max_length_src = max_length_src\n",
    "#         self.max_length_tgt = max_length_tgt\n",
    "#         self.pad_idx = pad_idx\n",
    "\n",
    "        \n",
    "#         # define the transformer module\n",
    "#         self.transformer = nn.Transformer(d_model = self.d_model, nhead = self.nhead, num_encoder_layers = self.enc_layer,\n",
    "#                                           num_decoder_layers = self.dec_layer, dim_feedforward=self.dim_forward,\n",
    "#                                           dropout = self.dropout, batch_first = True, bias = True, device = self.device)\n",
    "        \n",
    "#         # define the embedding for the ids and the position\n",
    "#         self.src_embedding = nn.Embedding(num_embeddings = self.input_size, embedding_dim = self.d_model)\n",
    "#         self.tgt_embedding = nn.Embedding(num_embeddings = self.output_size, embedding_dim = self.d_model)\n",
    "#         self.src_posembedding = nn.Embedding(num_embeddings = self.max_length_src, embedding_dim = self.d_model)\n",
    "#         self.tgt_posembedding = nn.Embedding(num_embeddings = self.max_length_tgt, embedding_dim = self.d_model)\n",
    "        \n",
    "#         # expand the hidden to the output size (same as the input vocabulary)\n",
    "#         self.deco_final_layer  = nn.Linear(self.d_model , self.input_size)\n",
    "        \n",
    "#     def forward(self, src, tgt, src_key_mask, tgt_key_mask):\n",
    "#         # embed the inputs\n",
    "#         src_embed = self.src_embedding(src) # src[N x T] -> [N x T x H]\n",
    "#         src_pos_embed = self.src_posembedding(torch.arange(self.max_length_src).to(self.device))\n",
    "#         src_total_embed = src_embed + src_pos_embed # add position embed\n",
    "        \n",
    "#         tgt_embed = self.tgt_embedding(tgt) # tgt [N x T] -> [N x T x H]\n",
    "#         tgt_pos_embed = self.tgt_embedding(torch.arange(self.max_length_tgt).to(self.device))\n",
    "#         tgt_total_embed = tgt_embed + tgt_pos_embed\n",
    "        \n",
    "#         # feed the embedding into the transformer\n",
    "#         # this is mostly used for autoregression, but never the less, we will set it. \n",
    "#         tgt_mask = nn.Transformer.generate_square_subsequent_mask(sz = self.max_length_tgt)\n",
    "#         tgt_seq_mask = (tgt_mask == float('-inf')).to(self.device)\n",
    "#         #src_seq_mask = (tgt_mask == float('-inf')).to(self.device)\n",
    "        \n",
    "#         # the mask provided by the hugging face is [1,1,1,0,0,0,0] -> [False, False, True]\n",
    "#         # TRUE values cannot participate in attention -  this seems to be correct\n",
    "#         src_key_mask = (src_key_mask.bool() != True).to(self.device)\n",
    "#         tgt_key_mask = (tgt_key_mask.bool() != True).to(self.device)\n",
    "        \n",
    "#         # run the transformer: this will have the output of [N, T, H]\n",
    "#         transformer_out = self.transformer(src = src_total_embed, tgt = tgt_total_embed,\n",
    "#                                            tgt_mask = tgt_seq_mask,\n",
    "#                                            src_key_padding_mask = src_key_mask , tgt_key_padding_mask = tgt_key_mask)\n",
    "        \n",
    "#         # run the transformer output through the final layer [N, T, H] -> [N, T, vocab]\n",
    "#         final_output = self.deco_final_layer(transformer_out)\n",
    "        \n",
    "#         return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state dictionary\n",
    "torch.save(model.state_dict(), \"saved_models/prior_model.pth\")\n",
    "\n",
    "\n",
    "# # # Load model\n",
    "# model = SmallTransformerModel(vocab_size, embedding_dim, hidden_dim, n_heads, n_layers, sequence_length)\n",
    "# model.load_state_dict(torch.load(\"model.pth\"))\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_sequence(model, start_token_idx = K+1, end_token_idx = K+2, max_len=50):\n",
    "#     model.eval()\n",
    "#     device = next(model.parameters()).device\n",
    "#     generated_sequence = [start_token_idx]  # Start with the [START] token\n",
    "    \n",
    "#     for _ in range(max_len - 1):  # Generate up to max_len tokens\n",
    "#         input_seq = torch.tensor([generated_sequence], dtype=torch.long).to(device)\n",
    "#         logits = model(input_seq)\n",
    "        \n",
    "#         # Get the most likely next token (argmax)\n",
    "#         next_token = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "        \n",
    "#         generated_sequence.append(next_token)\n",
    "        \n",
    "#         # Optionally, stop generation if [END] token is predicted\n",
    "#         if next_token == end_token_idx:\n",
    "#             break\n",
    "    \n",
    "#     return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(\n",
    "    model, \n",
    "    start_token_idx=K+1, \n",
    "    end_token_idx=K+2, \n",
    "    max_len=50, \n",
    "    temperature=0.8, \n",
    "    top_k=None\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    generated_sequence = [start_token_idx]  # Start with the [START] token\n",
    "\n",
    "    for _ in range(max_len - 1):  # Generate up to max_len tokens\n",
    "        input_seq = torch.tensor([generated_sequence], dtype=torch.long).to(device)\n",
    "        logits = model(input_seq)\n",
    "        \n",
    "        # Extract logits for the last token in the sequence\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            # Apply top-k sampling\n",
    "            next_token = top_k_sampling(next_token_logits, k=top_k)\n",
    "        else:\n",
    "            # Default to greedy decoding (argmax)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "        \n",
    "        generated_sequence.append(next_token)\n",
    "        \n",
    "        # Stop generation if [END] token is predicted\n",
    "        # if next_token == end_token_idx:\n",
    "        #     break\n",
    "\n",
    "    # Return the sequence excluding [START] and [END] tokens\n",
    "    return generated_sequence[1:] if generated_sequence[-1] == end_token_idx else generated_sequence[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 0, 23, 1, 18, 2, 66, 36, 28, 3, 21, 0, 20, 12, 19, 20, 12, 19, 20, 12, 19, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54]\n"
     ]
    }
   ],
   "source": [
    "generated_seq = generate_sequence(model)\n",
    "print(generated_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    ">> the model fails to converge :P"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
